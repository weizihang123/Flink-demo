
import com.guogan.wordCount.Bean.LoginEvent;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.IterativeCondition;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;

import java.util.List;
import java.util.Map;

/**
 * @author weixu
 * @create 2020-09-25 12:11
 */
public class Flink03_CEP_API {
    public static void main(String[] args) throws Exception {
        // 0.创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1.读取数据
        SingleOutputStreamOperator<LoginEvent> loginDS = env
                .readTextFile("input/LoginLog.csv")
                .map(new MapFunction<String, LoginEvent>() {
                    @Override
                    public LoginEvent map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new LoginEvent(
                                Long.valueOf(datas[0]),
                                datas[1],
                                datas[2],
                                Long.valueOf(datas[3])
                        );
                    }
                })
                .assignTimestampsAndWatermarks(
                        new BoundedOutOfOrdernessTimestampExtractor<LoginEvent>(Time.seconds(10)) {
                            @Override
                            public long extractTimestamp(LoginEvent element) {
                                return element.getEventTime() * 1000L;
                            }
                        }
                );
        KeyedStream<LoginEvent, Long> loginKS = loginDS.keyBy(r -> r.getUserId());
        //设置规则
        Pattern<LoginEvent, LoginEvent> failPS = Pattern.<LoginEvent>begin("start")
                .where(new IterativeCondition<LoginEvent>() {
                    @Override
                    public boolean filter(LoginEvent loginEvent, Context<LoginEvent> context) throws Exception {
                        return "fail".equals(loginEvent.getEventType());
                    }
                }).followedBy("followe")
                .where(new IterativeCondition<LoginEvent>() {
                    @Override
                    public boolean filter(LoginEvent loginEvent, Context<LoginEvent> context) throws Exception {
                        return "fail".equals(loginEvent.getEventType());
                    }
                }).within(Time.seconds(2));

        //应用规则
        PatternStream<LoginEvent> pattern = CEP.pattern(loginKS, failPS);
        //选择结果
        SingleOutputStreamOperator<String> select = pattern.select(new PatternSelectFunction<LoginEvent, String>() {
            @Override
            public String select(Map<String, List<LoginEvent>> map) throws Exception {
                LoginEvent firstFail = map.get("start").iterator().next();
                return "用户" + firstFail.getUserId() + "在2s内连续登陆失败2次，可能为恶意登陆！！！";
            }
        });
        select.print();
        env.execute();


    }
}


import com.guogan.wordCount.Bean.OrderEvent;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternSelectFunction;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.IterativeCondition;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;

import java.util.List;
import java.util.Map;

/**
 * @author weixu
 * @create 2020-09-26 15:48
 */
public class Flink05_Case_OrderTimeoutDetectWithCEP {
    public static void main(String[] args) throws Exception {
        // 0 执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1.读取数据，转成bean对象
        SingleOutputStreamOperator<OrderEvent> orderDS = env
                .readTextFile("input/OrderLog.csv")
                .map(new MapFunction<String, OrderEvent>() {
                    @Override
                    public OrderEvent map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new OrderEvent(
                                Long.valueOf(datas[0]),
                                datas[1],
                                datas[2],
                                Long.valueOf(datas[3])
                        );
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<OrderEvent>() {
                            @Override
                            public long extractAscendingTimestamp(OrderEvent element) {
                                return element.getEventTime() * 1000L;
                            }
                        }
                );
        KeyedStream<OrderEvent, Long> orderKS = orderDS.keyBy(r -> r.getOrderId());
        Pattern<OrderEvent, OrderEvent> pattern = Pattern.<OrderEvent>begin("create")
                .where(new IterativeCondition<OrderEvent>() {
                    @Override
                    public boolean filter(OrderEvent value, Context<OrderEvent> context) throws Exception {
                        return "create".equals(value.getEventType());
                    }
                }).followedBy("payEvent")
                .where(new IterativeCondition<OrderEvent>() {
                    @Override
                    public boolean filter(OrderEvent orderEvent, Context<OrderEvent> context) throws Exception {
                        return "pay".equals(orderEvent.getEventType());
                    }
                })
                .within(Time.seconds(15));
        PatternStream<OrderEvent> orderPS = CEP.pattern(orderKS, pattern);
        SingleOutputStreamOperator<String> resultDS = orderPS.select(new PatternSelectFunction<OrderEvent, String>() {
            @Override
            public String select(Map<String, List<OrderEvent>> map) throws Exception {
                OrderEvent createEvent = map.get("create").iterator().next();
                OrderEvent payEvent = map.get("payEvent").iterator().next();
                StringBuilder resultStr = new StringBuilder();
                resultStr
                        .append("订单ID:" + createEvent.getOrderId() + "\n")
                        .append("下单时间:" + createEvent.getEventTime() + "\n")
                        .append("支付时间:" + payEvent.getEventTime() + "\n")
                        .append("耗时:" + (payEvent.getEventTime() - createEvent.getEventTime()) + "秒\n")
                        .append("=======================================================================\n\n");
                return resultStr.toString();
            }
        });

        resultDS.print();
        env.execute();
    }
}


/**
 * 订单超时监测
 *
 * @author cjp
 * @version 1.0
 * @date 2020/9/18 16:32
 */
public class Flink06_Case_OrderTimeoutDetectWithCEP {
    public static void main(String[] args) throws Exception {
        // 0 执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1.读取数据，转成bean对象
        SingleOutputStreamOperator<OrderEvent> orderDS = env
                .readTextFile("input/OrderLog.csv")
                .map(new MapFunction<String, OrderEvent>() {
                    @Override
                    public OrderEvent map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new OrderEvent(
                                Long.valueOf(datas[0]),
                                datas[1],
                                datas[2],
                                Long.valueOf(datas[3])
                        );
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<OrderEvent>() {
                            @Override
                            public long extractAscendingTimestamp(OrderEvent element) {
                                return element.getEventTime() * 1000L;
                            }
                        }
                );

        // 2.处理数据：订单超时监控
        // 2.1 按照 统计维度 分组：订单
        KeyedStream<OrderEvent, Long> orderKS = orderDS.keyBy(order -> order.getOrderId());
        //TODO 2.2 CEP超时分析
        // 1. 定义规则
        Pattern<OrderEvent, OrderEvent> pattern = Pattern
                .<OrderEvent>begin("create")
                .where(new IterativeCondition<OrderEvent>() {
                    @Override
                    public boolean filter(OrderEvent value, Context<OrderEvent> ctx) throws Exception {
                        return "create".equals(value.getEventType());
                    }
                })
                .followedBy("pay")
                .where(new IterativeCondition<OrderEvent>() {
                    @Override
                    public boolean filter(OrderEvent value, Context<OrderEvent> ctx) throws Exception {
                        return "pay".equals(value.getEventType());
                    }
                })
                .within(Time.minutes(15));

        // 2. 使用规则
        PatternStream<OrderEvent> orderPS = CEP.pattern(orderKS, pattern);

        // 3. 获取匹配结果
        // TODO select 可以传三个参数
        // 1.第一个参数： 侧输出流的 OutputTag 对象
        // 2.超时数据的处理：对超时数据进行 xx 处理，放入 侧输出流
        // 3.匹配上的数据的处理： 对正常匹配上的数据进行 xx 处理，放入 主流
        // CEP只能处理正常情况，比如 系统存在异常、漏洞这些情况 没法发现
        OutputTag<String> timeoutTag = new OutputTag<String>("timeout") {
        };
        SingleOutputStreamOperator<String> resultDS = orderPS.select(timeoutTag, new TimeOutFunction(), new SelectFunction());

//        resultDS.print("result");
        resultDS.getSideOutput(timeoutTag).print("timeout");

        env.execute();
    }

    public static class TimeOutFunction implements PatternTimeoutFunction<OrderEvent, String> {

        @Override
        public String timeout(Map<String, List<OrderEvent>> map, long timeoutTimestamp) throws Exception {
            String keys = map.keySet().toString();
            String values = map.values().toString();
            StringBuilder timeoutStr = new StringBuilder();
            timeoutStr
                    .append("======================================================\n")
                    .append("map里所有的key=" + keys + "\n")
                    .append("map里所有的value=" + values)
                    .append("======================================================\n\n\n\n");
            return timeoutStr.toString();
        }
    }


    public static class SelectFunction implements PatternSelectFunction<OrderEvent, String> {

        @Override
        public String select(Map<String, List<OrderEvent>> map) throws Exception {
            OrderEvent createEvent = map.get("create").iterator().next();
            OrderEvent payEvent = map.get("pay").iterator().next();
            StringBuilder resultStr = new StringBuilder();
            resultStr
                    .append("订单ID:" + createEvent.getOrderId() + "\n")
                    .append("下单时间:" + createEvent.getEventTime() + "\n")
                    .append("支付时间:" + payEvent.getEventTime() + "\n")
                    .append("耗时:" + (payEvent.getEventTime() - createEvent.getEventTime()) + "秒\n")
                    .append("=======================================================================\n\n");
            return resultStr.toString();
        }
    }

}

import com.guogan.wordCount.Bean.OrderEvent;
import com.guogan.wordCount.Bean.TxEvent;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

/**
 * @author weixu
 * @create 2020-09-26 17:06
 */
public class Flink07_Case_OrderTxDetectWithIntervalJoin {
    public static void main(String[] args) throws Exception {
        // 0 执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(2);
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1.读取数据，转成bean对象
        SingleOutputStreamOperator<OrderEvent> orderDS = env
                .readTextFile("input/OrderLog.csv")
                .map(new MapFunction<String, OrderEvent>() {
                    @Override
                    public OrderEvent map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new OrderEvent(
                                Long.valueOf(datas[0]),
                                datas[1],
                                datas[2],
                                Long.valueOf(datas[3])
                        );
                    }
                })
                .assignTimestampsAndWatermarks(
                        new BoundedOutOfOrdernessTimestampExtractor<OrderEvent>(Time.seconds(10)) {
                            @Override
                            public long extractTimestamp(OrderEvent element) {
                                return element.getEventTime() * 1000L;
                            }
                        }
                );

        SingleOutputStreamOperator<TxEvent> txDS = env
                .readTextFile("input/ReceiptLog.csv")
                .map(new MapFunction<String, TxEvent>() {
                    @Override
                    public TxEvent map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new TxEvent(
                                datas[0],
                                datas[1],
                                Long.valueOf(datas[2])
                        );
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<TxEvent>() {
                            @Override
                            public long extractAscendingTimestamp(TxEvent element) {
                                return element.getEventTime() * 1000L;
                            }
                        }
                );
        // 2.处理数据：实时对账 监控
        //TODO Interval Join 实现
        // 2.1 按照 匹配的数据 分组：交易码
        KeyedStream<OrderEvent, String> orderKS = orderDS.keyBy(order -> order.getTxId());
        KeyedStream<TxEvent, String> txKS = txDS.keyBy(tx -> tx.getTxId());

        SingleOutputStreamOperator<String> resultDS = orderKS.intervalJoin(txKS).between(Time.seconds(-15), Time.seconds(15)).process(new JoinFunction());
        resultDS.print();
        env.execute();


    }
    public static class JoinFunction extends ProcessJoinFunction<OrderEvent,TxEvent,String> {
        @Override
        public void processElement(OrderEvent orderEvent, TxEvent txEvent, Context context, Collector<String> collector) throws Exception {
            if (orderEvent.getTxId().equals(txEvent.getTxId())) {
                collector.collect("订单" + orderEvent.getOrderId() + "对账成功");
            }
        }
    }
}


/**
 * @author weixu
 * @create 2020-09-25 9:47
 */
public class Flink_cep_API {
    public static void main(String[] args) throws Exception {
        // 0.创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        // 1.从文件读取数据
        SingleOutputStreamOperator<WaterSensor> sensorDS = env
                .readTextFile("input/sensor-data.log")
                .map(new MapFunction<String, WaterSensor>() {
                    @Override
                    public WaterSensor map(String value) throws Exception {
                        String[] datas = value.split(",");
                        return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
                    }
                })
                .assignTimestampsAndWatermarks(
                        new AscendingTimestampExtractor<WaterSensor>() {
                            @Override
                            public long extractAscendingTimestamp(WaterSensor element) {
                                return element.getTs() * 1000L;
                            }
                        }
                );
        //1.定义规则
        Pattern<WaterSensor, WaterSensor> pattern1 = Pattern.<WaterSensor>begin("pattern")
                .where(new IterativeCondition<WaterSensor>() {
                    @Override
                    public boolean filter(WaterSensor waterSensor, Context<WaterSensor> context) throws Exception {
                        return "sensor_1".equals(waterSensor.getId());
                    }
                })
                .followedBy("followedBy")
                .where(new IterativeCondition<WaterSensor>(){

                    @Override
                    public boolean filter(WaterSensor waterSensor, Context<WaterSensor> context) throws Exception {
                        return "sensor_1".equals(waterSensor.getId());
                    }
                })
                .within(Time.seconds(3));
        //2.使用规则
        PatternStream<WaterSensor> sensorPS = CEP.pattern(sensorDS, pattern1);
        
        //3.选择结果
        SingleOutputStreamOperator<String> result = sensorPS.select(new PatternSelectFunction<WaterSensor, String>() {
            @Override
            public String select(Map<String, List<WaterSensor>> map) throws Exception {

                String result = map.get("pattern").toString();
//                String next = map.get("next").toString();
//                return result+"-->"+next;
                return result;
            }

        });
        result.print("cep");
        env.execute();
    }
}
